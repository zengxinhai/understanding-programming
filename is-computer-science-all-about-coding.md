Author: Mark McCartin-Lim

Source: https://www.quora.com/Is-computer-science-all-about-coding-If-not-what-do-we-actually-learn-in-computer-science-in-the-university

---

## Classical computer science

- *Theoretical models of computation* — the idea of a computer existed before the machines physically existed, and you can mathematically prove theoretical limitations of different types of computation models.

- *Computer architecture* — the theoretical models describe functionality, but not physical implementation, so computer architecture adds a physical layer to the theoretical models, and is where you get into the physical components you put together to make a computer work.

- *Algorithms and notions of efficient computation* — once you have a model of computation, whether theoretical or physical, you want to solve problems with the computer. You design procedures for the computer, called algorithms, to solve different types of problems, and you want to be able to compare different algorithms in terms of their efficiency. So you’ll learn formal methods for analyzing and comparing efficiency. You’ll also learn different problem solving strategies that have been used in algorithm design.

- *System design* — each algorithm can be thought of as a specific task to solve a specific problem, but we often want computers to not only do one task but to be a complex system that can simultaneously handle many tasks. So you’ll learn about system design principles that have been used to engineer such complex systems. Specifically, resource management is a big issue. The computer has limited resources — the RAM is a resource, the file system is a resource, the screen is a resource, even the CPU and time itself is a resource. You also may have different guarantees for different kinds of tasks running on the system — certain tasks are time critical, certain tasks preempt other tasks, certain tasks are allowed to fail and others must always succeed, etc. So how do you manage the resources to ensure the guarantees you want? These principles are usually addressed to students by teaching them how to build a operating system like UNIX, but apply equally to building other kinds of systems like web servers and databases.

- *Network design* — once you have a bunch of systems, how do you get them communicating with each other? What kind of protocols do you want to use? How do you deal with congestion on the network? How do you deal with information loss and delays? The network itself can be thought of as a giant system, so in some sense, this is really just a more advanced extension of system design. A network is a system where communication is fundamental to resource management, and where additional tolerances must be made for rogue entities in the network that don’t behave the way they’re supposed to.

- *Programming language design and implementation* — How do you go about designing a new programming language, and what features do you want it to have? Then how do you create a compiler that parses that language and translates it into something a machine would understand? Often, this will be broken up into two separate courses. One course will focus on how computer scientists classify and model programming languages, the theory of programming languages. The other course will focus on building compilers — essentially the techniques to translate one formal language to another.

- *Artificial intelligence* — From the beginning of computer science, the earliest pioneers like Alan Turing envisioned computers as “thinking machines”. And since that time, computer scientists have been fascinated with the idea of giving computers intelligent behavior. A traditional AI course would teach you some of the techniques (probably not all) that computer scientists have used (with varying levels of success) to produce what some people may consider intelligent behavior. Of course, the notion of “intelligence” is very debatable, so an AI course would also cover the different metrics and criteria that computer scientists have proposed for evaluating artificial intelligence.

## Modern computer science

The topics listed under “Classical computer science” are ones that were established in the 60’s and 70’s, when the main applications of computer science were building **operating systems** and **compilers**. Since then, the applications of computer science has exploded into a near infinite number, which has introduced many new topics to computer science. Some of these topics build directly on top of the principles of the classical ones, and other topics borrow a lot of principles from other areas of math and science.

It’s worth pointing out that while all new topics of computer science introduced since the 70’s correspond to some application of computing, the reverse is not true. **Not all applications of computing create legitimate academic topics in computer science.**

And as soon as I’ve mentioned this, it opens a big can of worms because there’s room for debate on what’s a legitimate academic topic in computer science and what isn’t. I can tell you that a number of the topics other answers listed I don’t think should be considered topics of computer science, regardless of whether a CS department somewhere offers them or not.

But the whole reason for opening this can of worms is that students often confuse computer science with its applications, and assume the goal of computer science is to teach them about specific applications rather than the principles those applications require.

For instance, *“Building word processors”* might be an application of computer science, but pretty much no one would consider that a legitimate topic to include a computer science curriculum. To build a word processor, you’re going to need to use a lot of the principles of classical computer science, but there’s no new principles specific to word processor design to learn from — at least none that academics have studied. So it’s not a new topic, just an application of old ones.

Similarly, I wouldn’t consider *“Web development”* to be a real computer science topic either — despite the fact that some CS departments may offer this as a course due to popular demand. Sure, you might be learning something new, since maybe you didn’t know HTML, JavaScript or PHP before you took the course. But the reason it doesn’t pass the smell test, in my opinion, is because it’s not a topic where it’s possible to improve upon what you’re learning. If you’re taking a course in operating systems, you might one day come up with a way to improve on the system design principles you’re learning. That’s the scientific basis for computer science — all the problem solving principles you learn may one day be improved if someone comes up with better solutions. But with web development, you’re not learning a new scientific principle — you’re learning some arbitrary languages that someone else decided should be the language of the web.

On the other hand, a course in *“Designing interactive networked multimedia information systems”* could be a legitimate hypothetical computer science topic. The difference between this and the “Web development” course, is that while this course would also talk about how the World Wide Web works, since it’s the most well-known example of such a system, that’s not where the course stops. It’s a broader topic and there’s room for academic inquiry — is the WWW the best way to build a system like this, or could there be a better way? Now, you can look at it from different perspectives — social economies, information representation theory, virtual computing, etc. And those different perspectives give you some design principles you can think about.

Anyway, here are some other legitimate computer science topics you might run into: *Machine Learning, Natural-Language Processing, Computer Vision, Robotics, Graphical Rendering, Human-Computer Interaction, Database Systems, Cryptography, Computer Security, Computational Biology, Software Engineering, Automated Reasoning Systems, Simulation, Quantum Computing.*
